apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: raw-to-parquet-2
  namespace: spark
spec:
  type: Scala
  mode: cluster
  sparkVersion: 3.1.1
  image: rg.fr-par.scw.cloud/benchmark-spark/raw-to-parquet:hadoop-3.2.0
  imagePullPolicy: Always
  mainClass: devoxx.benchmark.RawToParquet
  mainApplicationFile: local:///opt/spark/examples/jars/raw-to-parquet-0.0.1-SNAPSHOT-shaded.jar
  hadoopConf:
    "fs.s3a.aws.credentials.provider": "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider"
    "mapreduce.outputcommitter.factory.scheme.s3a": "org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory"
    "spark.sql.sources.commitProtocolClass": "org.apache.spark.internal.io.cloud.PathOutputCommitProtocol"
    "spark.sql.parquet.output.committer.class": "org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter"
    "fs.s3a.committer.magic.enabled": "true"
    "fs.s3a.committer.name": "magic"
  driver:
    env:
      - name: REGION
        value: "fr-par"
      - name: INPUT
        value: "s3a://datalake-spark-benchmark/data/TPC-DS/10TB/store_sales"
      - name: OUTPUT
        value: "s3a://datalake-benchmark-spark/data/clean/TPC-DS/10TB/store_sales"
    envFrom:
      - secretRef:
          name: scw-secrets
    cores: 1
    memory: "1024m"
    hostNetwork: true
    labels:
      version: 3.1.1
    serviceAccount: spark-kube
  executor:
    serviceAccount: spark-kube
    env:
      - name: INPUT
        value: "s3a://datalake-spark-benchmark/data/TPC-DS/10TB/store_sales"
      - name: OUTPUT
        value: "s3a://datalake-benchmark-spark/data/clean/TPC-DS/10TB/store_sales"
    envFrom:
      - secretRef:
          name: scw-secrets
    cores: 3
    instances: 50
    memory: "16g"
